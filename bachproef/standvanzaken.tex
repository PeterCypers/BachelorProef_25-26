\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.
\section{Tips:}
Dit hoofdstuk bevat je literatuurstudie. De inhoud gaat verder op de inleiding, maar zal het onderwerp van de bachelorproef *diepgaand* uitspitten. De bedoeling is dat de lezer na lezing van dit hoofdstuk helemaal op de hoogte is van de huidige stand van zaken (state-of-the-art) in het onderzoeksdomein. Iemand die niet vertrouwd is met het onderwerp, weet nu voldoende om de rest van het verhaal te kunnen volgen, zonder dat die er nog andere informatie moet over opzoeken \autocite{Pollefliet2011}.

Je verwijst bij elke bewering die je doet, vakterm die je introduceert, enz.\ naar je bronnen. In \LaTeX{} kan dat met het commando \texttt{$\backslash${textcite\{\}}} of \texttt{$\backslash${autocite\{\}}}. Als argument van het commando geef je de ``sleutel'' van een ``record'' in een bibliografische databank in het Bib\LaTeX{}-formaat (een tekstbestand). Als je expliciet naar de auteur verwijst in de zin (narratieve referentie), gebruik je \texttt{$\backslash${}textcite\{\}}. Soms is de auteursnaam niet expliciet een onderdeel van de zin, dan gebruik je \texttt{$\backslash${}autocite\{\}} (referentie tussen haakjes). Dit gebruik je bv.~bij een citaat, of om in het bijschrift van een overgenomen afbeelding, broncode, tabel, enz. te verwijzen naar de bron. In de volgende paragraaf een voorbeeld van elk.

\textcite{Knuth1998} schreef een van de standaardwerken over sorteer- en zoekalgoritmen. Experten zijn het erover eens dat cloud computing een interessante opportuniteit vormen, zowel voor gebruikers als voor dienstverleners op vlak van informatietechnologie~\autocite{Creeger2009}.

Let er ook op: het \texttt{cite}-commando voor de punt, dus binnen de zin. Je verwijst meteen naar een bron in de eerste zin die erop gebaseerd is, dus niet pas op het einde van een paragraaf.

\section{AI \& LLMs}
\label{sec:aienllms}
%prominent note: de termen LLMs en AI gaan hierna door elkaar gebruikt worden, maar weet dat we het altijd zullen hebben over systemen die met drijfveer Large Language Modellen gebruiken, ook wanneer er over AI gesproken wordt
% note: keep this chapter verry theoretical (besides the sci-fi bit)
%intro

%this is part of the intro, but something needs to come before it
Een graaf met verzamelde data van posts op stackoverflow van gebruiker einpoklum deed recent de ronde op sociale media. Als we het aantal gebruikers kunnen afleiden uit het aantal posts op het platvorm dan ziet het een ergerlijke situatie uit, het gebruik van AI agents om te assisteren met het genereren van code en debuggen van errors zal zeker een beinvloedende factor zijn die bijdraagde tot de huidige staat van die website.

Section on Stackoverflow being replaced with AI as coders are using LLMs more and more to help with code problems
https://meta.stackoverflow.com/questions/437921/how-does-the-continued-decline-in-posts-since-may-25-influence-our-interpretati



\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{stackoverflowposts.png}
    \caption[Voorbeeld figuur.]{\label{fig:stackoverflowpostsdecline}Het aantal posts op StackOverflow vanaf de website opstart tot op heden.}
\end{figure}

%todo: make sure to correctly reference figures (possible the ref needs to be in the figure explenation - see Research Methods)


\section{AI van Science-Fiction naar Werkelijkheid}
\label{sec:aivanscience-fictionnaarwerkelijkheid}
% add the bit I wrote earlier

\subsection{Discriminative AI}

\subsection{Generative AI}
%refer to course1 -> all input text, so first transforming inputs via speechtotext, img to base64, natural language processing

\section{LLMs}

%todo: in this chapter explaining LLMs, Tokens & Context window, parameters, Size, ETC

%todo: check output of ollama show gemma3 -> explain things like 

%todo: snelheid van llms gemeten in tokens/s https://www.reddit.com/r/LocalLLaMA/comments/1miggb2/what_hardware_to_run_gptoss120b/

%todo: chapter about Tokens \& Pricing(can mention pricing, but pricing gets it's own chapter in Marktonderzoek) -expected in chapter to mention differences by provider and measure in per million tokens -> I have an image somewhere showing this for gpt or claude

%todo: put in chapter about context window
%https://www.ibm.com/think/topics/context-window ()in refs as Bergmann)
Eerst even wat uitleg over de \textit{context window}. Volgens \todo{reference  Bergmann} kan dit gezien worden als het werkgeheugen van een LLM. Dit bepaalt hoe lang het gesprek kan lopen (meerdere prompts) en bepaalt ook externe resources de LLM tegelijk can verwerken (documenten, codefragmenten, afbeeldingen, etc.).
Alles dat gebruikt werd bij een bepaalde sessie met een LLM komt in de context window terecht, alle voorgaande vragen(prompts) en gedeelde resources worden in rekening genomen wanneer de LLM een response terug geeft aan de gebruiker.


%todo: intro in chapter: Tokens, same reference as context window: Bergmann
Alle input wordt omgezet naar tokens die de LLM gebruikt om voorspellingen te maken die leiden tot het meest waardevolle antwoord. Wanneer de totale opgebouwde tokens de context-venster overschrijden dan moet de LLM samenvattingen beginnen maken ... \todo{schrijf verder, reference: Bergmann}

%todo: ubiquitousness / of AI commercial popularity of AI / integration of AI in much software / focus on integration (windows sundar pachai: interview he says something about going all in on AI - ??% of new windows-code is AI or vibe coded -> find sources and references for sundar pachai's statements and the ??% vibecoded windows 11)

%todo:

Een prachtig visueel uitwerking van de interne werking van een LLM kan men hier bekijken: \footnote{\href{https://bbycroft.net/llm}{https://bbycroft.net/llm}}


\section{Multimodal AI}

\section{Agentic AI}

\subsection{Tool gebruik}

\subsection{RAG}
% large section... RAG(Retrieval Augmented Generation)
% explain the different steps that make up RAG, at least 1 figure that shows the entire process
%explain some various applications/use-cases for rag
%forwards reference to vergel.-studie demo
%subsubsection for each part of RAG, including Vector Databases

% j J

Een probleem dat bestaat bij AI is dat er een beperkt context is dat deze kan
%todo: rewrite to NL
Big Picture Thinking Problem (DRY/maintainability/existing functions/codebase-context understanding/focus on solving everything in the AI-output)
AI agents have limited context understanding, one of the problems that can arise from the use of AI is that it doesn’t have big picture, project-wide scope of understanding, it doesn’t know that an interface exists to call somewhere else in the project. 

It will always be a problem because AI will always have some limit to how much additional context it can be given when prompted, it may increase if models continue to get stronger, but there will always be some limit.

There are areas where some progress has been made in solving this problem, like cursor and claude code which use …* to get project-wide context

*: %todo: look into what they exactly did, it likely predates RAG
%*RAG? -> todo: find out/look up, etc

\subsection{ReACT Agent}

\subsection{Multi-Agent Workflow}
%forwards reference to vergel.-studie demo

\subsection{Model Context Protocol(todo)}
%postmanmcp
