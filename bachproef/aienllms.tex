%%=============================================================================
%% AI \& LLMs
%%=============================================================================
\chapter{AI \& LLMs}%
\label{ch:aienllms}

%todo: in this chapter explaining LLMs, Tokens & Context window, parameters, Size, ETC

%todo: check output of ollama show gemma3 -> explain things like 

%todo: snelheid van llms gemeten in tokens/s https://www.reddit.com/r/LocalLLaMA/comments/1miggb2/what_hardware_to_run_gptoss120b/

%todo: chapter about Tokens \& Pricing -expected in chapter to mention differences by provider and measure in per million tokens -> I have an image somewhere showing this for gpt or claude

%todo: put in chapter about context window
%https://www.ibm.com/think/topics/context-window ()in refs as Bergmann)
Eerst even wat uitleg over de \textit{context window}. Volgens \todo{reference  Bergmann} kan dit gezien worden als het werkgeheugen van een LLM. Dit bepaalt hoe lang het gesprek kan lopen (meerdere prompts) en bepaalt ook externe resources de LLM tegelijk can verwerken (documenten, codefragmenten, afbeeldingen, etc.).
Alles dat gebruikt werd bij een bepaalde sessie met een LLM komt in de context window terecht, alle voorgaande vragen(prompts) en gedeelde resources worden in rekening genomen wanneer de LLM een response terug geeft aan de gebruiker.


%todo: intro in chapter: Tokens, same reference as context window: Bergmann
Alle input wordt omgezet naar tokens die de LLM gebruikt om voorspellingen te maken die leiden tot het meest waardevolle antwoord. Wanneer de totale opgebouwde tokens de context-venster overschrijden dan moet de LLM samenvattingen beginnen maken ... \todo{schrijf verder, reference: Bergmann}